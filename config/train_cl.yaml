includes: [global.yaml, petl.yaml,colossalai_strategy.yaml]

# one of record lmdb arrow_stream arrow_file,parquet, 超大数据集可以使用 lmdb , 注 lmdb 存储空间比record大
data_backend: parquet


# 目前仅ddp 支持lora
# one of ddp,gemini,zero2,zero2_cpu,3d
strategy: ddp

output_dir: ./outputs_cl
overwrite_output_dir: true
num_train_epochs: 20
max_steps: -1
save_safetensors: false
save_strategy: steps
save_steps: 1000
save_total_limit: 10
seed: 42
fp16: true
do_train: true
train_file:
- ../data/*.json

do_eval: false
do_predict: false
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 1
evaluation_strategy: 'no'
eval_steps: 100

# 优化器，如果策略使用 gemini , 则 optim one of adam_hybrid_cl,adam_cpu_cl,adam_fused_cl
# 如果策略使用非 gemini ,则 optim one of follow
# one of adam_hybrid_cl,adam_cpu_cl,adam_fused_cl,lamb,adamw_hf,adamw,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_apex_fused,
# adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion,lion_8bit,lion_32bit,
# paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,
# lamb_fused_dp adagrad_cpu_dp adam_cpu_dp adam_fused_dp

optim: adam_hybrid_cl

# one of linear,cosine,cosine_with_restarts,polynomial,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau
lr_scheduler_type: cosine
torch_compile: false
learning_rate: 2.0e-05
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
weight_decay: 0.0
warmup_ratio: 0.03
logging_strategy: steps
logging_steps: 10
tf32: false
gradient_checkpointing: false
max_seq_length: 512
max_target_length: 100

do_lower_case: null
use_fast_tokenizer: false
dataloader_drop_last: true
dataloader_pin_memory: true
dataloader_num_workers: 0
log_level: info
